{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain ReadOps Assignment\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85673b0e-2cb3-4a35-9e5f-abe4cc54207c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- A catalog named telecom_catalog_assign\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "-- A database schema landing_zone\n",
    "create database if not exists telecom_catalog_assign.landing_zone;\n",
    "-- A volume using schema landing_vol\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e942cf3-8455-45c2-9333-650fd36b69f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folders = [\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"\n",
    "]\n",
    "for f in folders:\n",
    "    dbutils.fs.mkdirs(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93e01bdc-3904-47ec-944f-208de949545e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):\n",
    "--a. Volume vs DBFS/FileStore\n",
    "--b. Why production teams prefer Volumes for regulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102b8f91-3c40-4497-91ae-d40b89bb71b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#data csv files sto be used next are below.\n",
    "customer_csv = \"\"\"101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\"\"\"\n",
    "usage_tsv = \"\"\"customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\"\"\"\n",
    "tower_logs_region1 = \"\"\"event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "21231906-990c-4f10-b9a1-944131480a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#Databricks utility command used to create using '.put'\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", customer_csv, True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", usage_tsv,True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",tower_logs_region1,True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv\",tower_logs_region1,True)\n",
    "#show folder using '.ls' in filesystem\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a43a88-b376-40ee-88a6-44b53bee113c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"event\":272},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766027682062}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 read all towers using PathGlobFilter, multiple paths, recursive lookup\n",
    "tower_logs_paths = [\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\"]\n",
    "csv_df1 = spark.read.csv(tower_logs_paths,header=True,inferSchema=True,sep=\"|\",pathGlobFilter=\"*.csv\",recursiveFileLookup=True)\n",
    "display(csv_df1)\n",
    "#2 Demonstrate these 3 reads separately: Using pathGlobFilter Using list of paths in spark.read.csv([path1, path2]) Using .option(\"recursiveFileLookup\",\"true\")\n",
    "csv_df2 = spark.read.option(\"header\",\"True\")\\\n",
    "    .option(\"sep\", \"|\")\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .option(\"recursiveFileLookup\", True)\\\n",
    "    .option(\"pathGlobFilter\", \"*.csv\")\\\n",
    "    .csv([\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"])\n",
    "display(csv_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d427dc-4f71-4b95-adcc-025a882fb116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%python\n",
    "\n",
    "#1Customer read uisng hearder and inferschema false\n",
    "custdf =spark.read.options(header=\"False\",inferSchema=\"false\",sep=\",\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "custdf.show()\n",
    "\n",
    "#2change the header or inferSchema with true/false?\n",
    "chngdf=  spark.read.options(header=\"True\",inferSchema=\"True\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "chngdf.show()\n",
    "\n",
    "# when we give header as true it takes the first row as column-header, if false by defaultly it will show the column name like c0, c1, c2 ...., \n",
    "# inferschema by default its  false and it wil identify and show all the column data type as string (default), if True scan the column and show the datatype of the column.\n",
    "\n",
    "#3 Age contains both integer and string value so it is considerign the age column as string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc6dd1d-19b1-4595-8cd7-4313db77a178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,TimestampType\n",
    "#5.1\n",
    "custdf =spark.read.options(header=\"False\",inferSchema=\"false\",sep=\",\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "custdf.show()\n",
    "custdf.toDF(\"Id\",\"Name\",\"Age\",\"Location\",\"SimType\").show()\n",
    "\n",
    "#5.2\n",
    "custom_schema = StructType([StructField(\"cust_id\",IntegerType(),False),StructField(\"voi_mins\",IntegerType(),True),StructField(\"data_mb\",IntegerType(),True),StructField(\"sms_count\",IntegerType(),True)])\n",
    "\n",
    "custom_df1=spark.read.schema(custom_schema).options(header=\"True\", sep=\"\\t\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "custom_df1.show()\n",
    "\n",
    "custom_df1.printSchema()\n",
    "\n",
    "#5.3\n",
    "tower_schema = StructType([\n",
    "    StructField(\"eve_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"tower_id\", StringType(), True),\n",
    "    StructField(\"signal_strength\", IntegerType(), True),\n",
    "    StructField(\"event_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "tower_df1=spark.read.schema(tower_schema).options(header=\"True\", sep=\"|\" ,pathGlobFilter=\"*.csv\",recursiveFileLookup=\"True\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "tower_df1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. More to come (stay motivated)...."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6088836288958888,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3_read_write_usecases telecom_Hema",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
